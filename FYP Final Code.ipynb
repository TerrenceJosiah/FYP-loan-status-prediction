{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Comparative Study of Predicting Loan Status of a Lending Company using Various Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows the code for my Final Year Project: A Comparative Study of Predicting Loan Status of a Lending Company using Various Machine Learning Algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from xgboost import XGBRFClassifier, XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from pandas_profiling import ProfileReport\n",
    "from pandas_dq import dq_report, Fix_DQ\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, precision_score, roc_curve, recall_score, auc, f1_score, cohen_kappa_score, matthews_corrcoef\n",
    "from imblearn.over_sampling import *\n",
    "from imblearn.under_sampling import *\n",
    "from imblearn.combine import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.discriminant_analysis import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.neural_network import *\n",
    "from sklearn.cluster import *\n",
    "from sklearn.mixture import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.multiclass import *\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import svm,model_selection, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "\n",
    "# Set options for displaying data\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 400)\n",
    "sns.set(style = \"whitegrid\")\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set(rc = {'figure.figsize':(3,3)})\n",
    "sns.set_style('whitegrid')\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action = 'ignore', category = DataConversionWarning)\n",
    "warnings.filterwarnings(action = 'ignore', category = FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_data = pd.read_csv(\"accepted_2007_to_2018Q4.csv\", low_memory = False)\n",
    "df = pd.DataFrame(acc_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Data Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Automated EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile = ProfileReport(df)\n",
    "# profile.to_file('Terrence FYP EDA.html')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nul = df.isnull().mean().sort_values()\n",
    "\n",
    "nul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nul = nul[nul>0.3]\n",
    "\n",
    "nul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nul_col = nul.sort_values(ascending = False).index\n",
    "\n",
    "nul_col"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dropping Columns with Missing Values > 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(nul_col, axis = 1)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['id', 'url', 'loan_status'], axis = 1, inplace = True)\n",
    "data.drop(['grade', 'emp_title'], axis = 1, inplace = True)\n",
    "data.drop(['title', 'zip_code'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(how = 'all')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_col = ['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d']\n",
    "\n",
    "for value in date_col:\n",
    "    data[value + '_month'] = data[value].apply(lambda x: x[0:3] if isinstance(x, str) else x)\n",
    "    data[value + '_year'] = data[value].apply(lambda x: x[-4:] if isinstance(x, str) else x)\n",
    "\n",
    "data.drop(date_col, axis = 1, inplace = True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sampling to reduce work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac = 0.45, axis = 0, random_state = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_splitting(df, target_col, test_size):\n",
    "    global X, y, X_train, X_test, y_train, y_test\n",
    "    X = df.loc[:, df.columns != target_col]\n",
    "    y = df.loc[:, target_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = 0)\n",
    "\n",
    "data_splitting(data, 'sub_grade', 0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Label Encoding Before Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train\n",
    "\n",
    "X_train_categorical = [feature for feature in X_train.columns if X_train[feature].dtype == \"O\"]\n",
    "\n",
    "for col in X_train_categorical:\n",
    "    X_train[col] = le.fit_transform(X_train[col])\n",
    "\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test\n",
    "\n",
    "X_test_categorical = [feature for feature in X_test.columns if X_test[feature].dtype == \"O\"]\n",
    "\n",
    "for col in X_test_categorical:\n",
    "    X_test[col] = le.fit_transform(X_test[col])\n",
    "\n",
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train\n",
    "\n",
    "y_train = le.fit_transform(y_train)\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test\n",
    "\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checking Null Values and Perform Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column\n",
    "def missing_values_table(df):\n",
    "  # Total missing values\n",
    "  mis_val = df.isnull().sum()\n",
    "\n",
    "  # Percentage of missing values\n",
    "  mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "\n",
    "  # Make a table with the results\n",
    "  mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "\n",
    "  # Rename the columns\n",
    "  mis_val_table_ren_columns = mis_val_table.rename(\n",
    "  columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "\n",
    "  # Sort the table by percentage of missing descending\n",
    "  mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "      mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "  '% of Total Values', ascending=False).round(1)\n",
    "\n",
    "  # Print some summary information\n",
    "  print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "      \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "        \" columns that have missing values.\")\n",
    "\n",
    "  # Return the dataframe with missing information\n",
    "  return mis_val_table_ren_columns\n",
    "\n",
    "missing_values_table(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "x_train_col = X_train.columns\n",
    "x_test_col = X_test.columns\n",
    "\n",
    "imputer = SimpleImputer(strategy = 'mean')\n",
    "X_train = pd.DataFrame(imputer.fit_transform(X_train))\n",
    "X_test = pd.DataFrame(imputer.transform(X_test))\n",
    "\n",
    "X_train.columns = x_train_col\n",
    "X_test.columns = x_test_col"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "lr_test_accuracy = lr.score(X_test, y_test)\n",
    "\n",
    "print(\"Logistic Regression Test Accuracy:\", lr_test_accuracy)\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gnb = gnb.predict(X_test)\n",
    "gnb_test_accuracy = gnb.score(X_test, y_test)\n",
    "\n",
    "print(\"Gaussian Naive Bayes Test Accuracy:\", gnb_test_accuracy)\n",
    "print(classification_report(y_test, y_pred_gnb))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "dt_test_accuracy = dt.score(X_test, y_test)\n",
    "\n",
    "print(\"Decision Tree Classifier Test Accuracy:\", dt_test_accuracy)\n",
    "print(classification_report(y_test, y_pred_dt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "rf_test_accuracy = rf.score(X_test, y_test)\n",
    "\n",
    "print(\"Random Forest Classifier Test Accuracy:\", rf_test_accuracy)\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = BaggingClassifier()\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "y_pred_bag = bag.predict(X_test)\n",
    "bag_test_accuracy = bag.score(X_test, y_test)\n",
    "\n",
    "print(\"Bagging Classifier Test Accuracy:\", bag_test_accuracy)\n",
    "print(classification_report(y_test, y_pred_bag))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier()\n",
    "\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "y_pred_ada = ada.predict(X_test)\n",
    "ada_test_accuracy = ada.score(X_test, y_test)\n",
    "\n",
    "print(\"Ada Boost Classifier Test Accuracy:\", ada_test_accuracy)\n",
    "print(classification_report(y_test, y_pred_ada))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Light Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb = LGBMClassifier()\n",
    "\n",
    "lgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lgb = lgb.predict(X_test)\n",
    "lgb_test_accuracy = lgb.score(X_test, y_test)\n",
    "\n",
    "print(\"Light Gradient Boosting Classifier Test Accuracy:\", lgb_test_accuracy)\n",
    "print(classification_report(y_test, y_pred_lgb))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extreme Gradient Boosting Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "xgb_test_accuracy = xgb.score(X_test, y_test)\n",
    "\n",
    "print(\"Extreme Gradient Boosting Classifier Test Accuracy:\", xgb_test_accuracy)\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train.columns  # Replace X_train with your actual feature data\n",
    "\n",
    "# Get the absolute values of the coefficients for feature importances\n",
    "coefficients = np.abs(lr.coef_[0])\n",
    "\n",
    "# Sort the feature importance scores and feature names in descending order\n",
    "sorted_indices = np.argsort(coefficients)[::-1]\n",
    "sorted_feature_importance = coefficients[sorted_indices]\n",
    "sorted_feature_names = feature_names[sorted_indices]\n",
    "\n",
    "# Limit to top 5 feature importances\n",
    "top_n = 5\n",
    "sorted_feature_importance = sorted_feature_importance[:top_n]\n",
    "sorted_feature_names = sorted_feature_names[:top_n]\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.barplot(x=sorted_feature_importance, y=sorted_feature_names)\n",
    "plt.title('Logistic Regression Feature Importance')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance based on the standard deviation of each feature\n",
    "feature_importance = np.std(gnb.theta_, axis=0)\n",
    "\n",
    "# Get the feature names from your input data or a predefined list\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Sort the feature importance scores and feature names in descending order\n",
    "sorted_indices = np.argsort(feature_importance)[::-1]\n",
    "sorted_feature_importance = feature_importance[sorted_indices]\n",
    "sorted_feature_names = feature_names[sorted_indices]\n",
    "\n",
    "# Limit to top 5 feature importances\n",
    "top_n = 5\n",
    "sorted_feature_importance = sorted_feature_importance[:top_n]\n",
    "sorted_feature_names = sorted_feature_names[:top_n]\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.barplot(x = sorted_feature_importance, y = sorted_feature_names)\n",
    "plt.title('Gaussian Naive Bayes Feature Importance')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance based on the standard deviation of each feature\n",
    "feature_importance = dt.feature_importances_\n",
    "\n",
    "# Get the feature names from your input data or a predefined list\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Sort the feature importance scores and feature names in descending order\n",
    "sorted_indices = np.argsort(feature_importance)[::-1]\n",
    "sorted_feature_importance = feature_importance[sorted_indices]\n",
    "sorted_feature_names = feature_names[sorted_indices]\n",
    "\n",
    "# Limit to top 5 feature importances\n",
    "top_n = 5\n",
    "sorted_feature_importance = sorted_feature_importance[:top_n]\n",
    "sorted_feature_names = sorted_feature_names[:top_n]\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.barplot(x = sorted_feature_importance, y = sorted_feature_names)\n",
    "plt.title('Decision Tree Feature Importance')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance based on the standard deviation of each feature\n",
    "feature_importance = rf.feature_importances_\n",
    "\n",
    "# Get the feature names from your input data or a predefined list\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Sort the feature importance scores and feature names in descending order\n",
    "sorted_indices = np.argsort(feature_importance)[::-1]\n",
    "sorted_feature_importance = feature_importance[sorted_indices]\n",
    "sorted_feature_names = feature_names[sorted_indices]\n",
    "\n",
    "# Limit to top 5 feature importances\n",
    "top_n = 5\n",
    "sorted_feature_importance = sorted_feature_importance[:top_n]\n",
    "sorted_feature_names = sorted_feature_names[:top_n]\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.barplot(x = sorted_feature_importance, y = sorted_feature_names)\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bootstrap Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance based on the standard deviation of each feature\n",
    "feature_importance /= len(bag.estimators_)\n",
    "\n",
    "# Get the feature names from your input data or a predefined list\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Sort the feature importance scores and feature names in descending order\n",
    "sorted_indices = np.argsort(feature_importance)[::-1]\n",
    "sorted_feature_importance = feature_importance[sorted_indices]\n",
    "sorted_feature_names = feature_names[sorted_indices]\n",
    "\n",
    "# Limit to top 5 feature importances\n",
    "top_n = 5\n",
    "sorted_feature_importance = sorted_feature_importance[:top_n]\n",
    "sorted_feature_names = sorted_feature_names[:top_n]\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.barplot(x = sorted_feature_importance, y = sorted_feature_names)\n",
    "plt.title('Bootstrap Aggregating Feature Importance')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance based on the standard deviation of each feature\n",
    "feature_importance = ada.feature_importances_\n",
    "\n",
    "# Get the feature names from your input data or a predefined list\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Sort the feature importance scores and feature names in descending order\n",
    "sorted_indices = np.argsort(feature_importance)[::-1]\n",
    "sorted_feature_importance = feature_importance[sorted_indices]\n",
    "sorted_feature_names = feature_names[sorted_indices]\n",
    "\n",
    "# Limit to top 5 feature importances\n",
    "top_n = 5\n",
    "sorted_feature_importance = sorted_feature_importance[:top_n]\n",
    "sorted_feature_names = sorted_feature_names[:top_n]\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.barplot(x = sorted_feature_importance, y = sorted_feature_names)\n",
    "plt.title('Adaptive Boosting Feature Importance')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Light Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance based on the standard deviation of each feature\n",
    "feature_importance = lgb.feature_importances_\n",
    "\n",
    "# Get the feature names from your input data or a predefined list\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Sort the feature importance scores and feature names in descending order\n",
    "sorted_indices = np.argsort(feature_importance)[::-1]\n",
    "sorted_feature_importance = feature_importance[sorted_indices]\n",
    "sorted_feature_names = feature_names[sorted_indices]\n",
    "\n",
    "# Limit to top 5 feature importances\n",
    "top_n = 5\n",
    "sorted_feature_importance = sorted_feature_importance[:top_n]\n",
    "sorted_feature_names = sorted_feature_names[:top_n]\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.barplot(x = sorted_feature_importance, y = sorted_feature_names)\n",
    "plt.title('Light Gradient Boosting Feature Importance')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance based on the standard deviation of each feature\n",
    "feature_importance = xgb.feature_importances_\n",
    "\n",
    "# Get the feature names from your input data or a predefined list\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Sort the feature importance scores and feature names in descending order\n",
    "sorted_indices = np.argsort(feature_importance)[::-1]\n",
    "sorted_feature_importance = feature_importance[sorted_indices]\n",
    "sorted_feature_names = feature_names[sorted_indices]\n",
    "\n",
    "# Limit to top 5 feature importances\n",
    "top_n = 5\n",
    "sorted_feature_importance = sorted_feature_importance[:top_n]\n",
    "sorted_feature_names = sorted_feature_names[:top_n]\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.barplot(x = sorted_feature_importance, y = sorted_feature_names)\n",
    "plt.title('Extreme Gradient Boosting Feature Importance')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba564c632e137132909a14204f64f299474f9aad007fd23fc13930ab5cadda97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
